_target_: peft.LoraConfig
_convert_: "all"
r: 16 # 64, 256?
lora_alpha: 16
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "down_proj", "up_proj", "gate_proj"]
lora_dropout: 0.05
bias: "none"
use_rslora: True
task_type: "CAUSAL_LM"
modules_to_save: null
